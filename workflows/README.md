# Workflow Engine for Workflow Use

The Python workflow engine is the core processing component that converts browser recordings into executable, LLM-enhanced workflows with deterministic automation capabilities.

## Architecture Overview

Built with **Python**, **FastAPI**, **Browser Use**, **LangChain**, and **Playwright**, the workflow engine provides intelligent workflow generation, robust execution, and seamless integration with the browser extension.

### Core Services

#### 1. RecordingService (`workflow_use/recorder/service.py`)
**Browser Recording Coordinator** - Manages browser sessions and captures workflow data from the extension.

**Core Responsibilities:**
- **Browser Management**: Launches Chromium with extension pre-loaded using Playwright
- **HTTP Event Server**: FastAPI server accepting real-time events from extension (port 7331)
- **Session Lifecycle**: Manages recording start/stop and browser lifecycle
- **Event Queue Processing**: Asynchronous event processing pipeline
- **Workflow Capture**: Converts session events to WorkflowDefinitionSchema

**Event Processing Pipeline:**
```python
# Event Reception Flow
Extension HTTP POST â†’ _handle_event_post() â†’ Event Queue â†’ _process_event_queue()
                  â†“
HTTP Events: WORKFLOW_UPDATE, RECORDING_STARTED, RECORDING_STOPPED
                  â†“
Final Workflow: WorkflowDefinitionSchema with structured steps
```

**Key Methods:**
```python
async def capture_workflow() -> WorkflowDefinitionSchema:
    # 1. Launch browser with extension
    # 2. Start FastAPI server for event reception  
    # 3. Process events in real-time
    # 4. Return final workflow on recording stop

async def _handle_event_post(event_data: RecorderEvent):
    # Process HTTP events from extension
    # Store latest workflow updates
    # Queue events for processing
```

#### 2. BuilderService (`workflow_use/builder/service.py`)
**LLM-Powered Workflow Generator** - Converts raw recordings into optimized, executable workflows.

**Core Responsibilities:**
- **LLM Analysis**: Uses GPT-4o to analyze recorded events and generate structured workflows
- **Vision Integration**: Processes screenshots for visual context understanding
- **DOM Content Refinement**: Converts natural language extraction rules to precise XPath selectors
- **Event Optimization**: Filters redundant events and optimizes step sequences
- **Schema Validation**: Ensures output conforms to WorkflowDefinitionSchema

**Workflow Building Pipeline:**
```python
# Workflow Generation Flow
Raw Recording â†’ LLM Analysis â†’ Event Filtering â†’ DOM Content Refinement â†’ Final Workflow
```

**Key Features:**
```python
# Optimized LLM-Powered Analysis with Caching
async def build_workflow_from_path(recording_path: str) -> WorkflowDefinitionSchema:
    # 1. Load and validate recording
    # 2. Filter redundant navigation events
    # 3. Process DOM content extraction with unified LLM invocation
    # 4. Generate input schema for workflow reusability
    # 5. Create final optimized workflow using cached actions metadata

# DOM Content Extraction Enhancement (Synchronous)
def _process_dom_content_marking(self, steps: List[Any]) -> List[Any]:
    # Convert natural language extraction rules to precise selectors
    # Use unified JSON parsing with markdown cleanup
    # Create robust XPath selectors with validation and normalization
    
# Unified LLM Invocation with Fallback
async def _invoke_llm_with_fallback(self, messages: list) -> WorkflowDefinitionSchema:
    # Single point for all LLM invocations with structured output
    # Graceful fallback to manual JSON parsing
    # Unified error handling and response processing
```

#### 3. WorkflowController (`workflow_use/controller/service.py`)
**Workflow Execution Engine** - Executes workflows with deterministic actions and intelligent fallbacks.

**Core Responsibilities:**
- **Step Execution**: Deterministic browser automation with multiple selector strategies
- **Content Extraction**: LLM-powered page content extraction with structured output
- **Fallback Mechanisms**: Failed deterministic steps automatically fall back to Browser Use agent
- **DOM Operations**: Advanced element selection with XPath, CSS, and visual targeting

**Execution Flow:**
```python
# Workflow Execution Pipeline
Workflow JSON â†’ Step Processing â†’ Browser Actions â†’ Content Extraction â†’ Results
                    â†“
Deterministic Steps: click, input, navigation, scroll
                    â†“
Agentic Steps: agent (Browser Use), extract_page_content (LLM)
```

**Key Capabilities:**
```python
# Unified Element Action Execution with Performance Optimization
async def _execute_element_action(self, browser_session, params, action_func, success_msg, error_msg):
    # Centralized element selection and action execution
    # Consistent error handling across all browser actions
    # Optimized timeout handling and logging
    # Support for custom action functions via lambda/callable patterns

# Streamlined Browser Actions (click, input, select_change, key_press)
async def click(params: ClickElementDeterministicAction, browser_session: Browser):
    return await self._execute_element_action(
        browser_session, params, lambda locator: locator.click(force=True),
        'ðŸ–±ï¸ Clicked element', 'Failed to click element'
    )
    # Reduced from ~20 lines to 4 lines per action
    # Unified error handling and logging

# Advanced DOM Content Extraction with Unified XPath Processing
async def extract_dom_content(params: DOMExtractionAction, browser_session: Browser):
    # Native XPath evaluation with document.evaluate()
    # Unified XPath query method for both page and element contexts
    # Intelligent XPath normalization (id() â†’ //*[@id='...'])
    # Cached fallback selector generation with @lru_cache

# LLM-Powered Page Content Extraction
async def extract_page_content(params: PageExtractionAction, browser_session: Browser):
    # Markdown conversion with iframe content inclusion
    # Structured output with Pydantic validation
    # Context-aware content understanding with goal-based extraction
```

## Data Flow Architecture

### Phase 1: Recording Capture
```
Browser Extension â†’ HTTP Events â†’ RecordingService
â”œâ”€â”€ Real-time event streaming
â”œâ”€â”€ Session management
â””â”€â”€ Workflow data aggregation
```

### Phase 2: Workflow Building
```
RecordingService â†’ Raw Recording â†’ BuilderService â†’ LLM Analysis â†’ Optimized Workflow
â”œâ”€â”€ Event filtering and deduplication
â”œâ”€â”€ DOM content extraction refinement
â”œâ”€â”€ Vision-enhanced understanding
â””â”€â”€ Input schema generation
```

### Phase 3: Workflow Execution
```
Workflow JSON â†’ WorkflowController â†’ Browser Automation â†’ Results
â”œâ”€â”€ Deterministic step execution
â”œâ”€â”€ Agentic fallback mechanisms
â””â”€â”€ Structured content extraction
```

## Key Data Structures

### Recording Schema
```python
# Event Types (from Extension)
HttpWorkflowUpdateEvent: {
    type: "WORKFLOW_UPDATE",
    payload: WorkflowDefinitionSchema
}

HttpRecordingStoppedEvent: {
    type: "RECORDING_STOPPED", 
    payload: RecordingStatusPayload
}
```

### Workflow Schema
```python
# Step Types
ClickStep: {
    type: "click",
    xpath: str,
    cssSelector: str,
    elementTag: str
}

InputStep: {
    type: "input", 
    xpath: str,
    value: str
}

AgentTaskWorkflowStep: {
    type: "agent",
    task: str,
    max_steps: int
}

DOMContentExtractionStep: {
    type: "extract_dom_content",
    containerXpath: str,
    fields: List[ExtractionField]
}

# Top-level Workflow
WorkflowDefinitionSchema: {
    name: str,
    description: str,
    version: str,
    steps: List[WorkflowStep],
    input_schema: List[WorkflowInputSchemaDefinition]
}
```

## Development Commands

### Environment Setup
```bash
# Install dependencies with uv
uv sync

# Activate virtual environment (REQUIRED)
source .venv/bin/activate

# Install browser automation
playwright install chromium
```

### CLI Commands
```bash
# Record new workflow
python cli.py create-workflow

# Run workflow with variables
python cli.py run-workflow examples/example.workflow.json

# Run workflow as tool with natural language
python cli.py run-as-tool examples/example.workflow.json --prompt "fill form with data"

# Launch web UI
python cli.py launch-gui
```

### Backend API
```bash
# Start FastAPI server for web UI
uvicorn backend.api:app --reload
```

### Code Quality
```bash
# Lint Python code
ruff check

# Format Python code  
ruff format
```

## Service Integration

### 1. RecordingService â†” Extension
```python
# HTTP API Integration
@app.post("/event")
async def _handle_event_post(event_data: RecorderEvent):
    # Real-time event processing from browser extension
    # Workflow data aggregation and storage
    return {"status": "accepted"}
```

### 2. BuilderService â†” RecordingService
```python
# Workflow Enhancement Pipeline
recording_data = await recording_service.capture_workflow()
enhanced_workflow = await builder_service.build_workflow_from_path(recording_data)
```

### 3. WorkflowController â†” Browser Use
```python
# Intelligent Automation Integration  
from browser_use import Browser, BrowserController

class WorkflowController(BrowserController):
    # Enhanced browser automation with workflow-specific features
    # Seamless integration with Browser Use agent capabilities
```

## Advanced Features

### 1. Unified Element Action Architecture
**Performance-Optimized Browser Automation** with centralized action execution and reduced code duplication.

```python
# Before Optimization - Repetitive Pattern (5 similar methods, ~20-30 lines each)
async def click(params, browser_session):
    page = await browser_session.get_current_page()
    original_selector = params.cssSelector
    try:
        locator, selector_used = await get_best_element_handle(...)
        await locator.click(force=True)
        msg = f'ðŸ–±ï¸ Clicked element...'
        logger.info(msg)
        return ActionResult(extracted_content=msg, include_in_memory=True)
    except Exception as e:
        error_msg = f'Failed to click element...'
        logger.error(error_msg)
        raise Exception(error_msg)

# After Optimization - Unified Pattern (DRY principle applied)
async def click(params, browser_session):
    return await self._execute_element_action(
        browser_session, params, lambda locator: locator.click(force=True),
        'ðŸ–±ï¸ Clicked element', 'Failed to click element'
    )
```

### 2. Optimized LLM-Powered Workflow Building
**Performance-Enhanced Analysis** with caching, unified JSON parsing, and intelligent workflow generation.

```python
# Optimized LLM Configuration with Caching
class BuilderService:
    def __init__(self, llm: BaseChatModel):
        self.llm = self._configure_structured_llm(llm)
        self._actions_markdown_cache = None  # Performance optimization
    
    @property 
    def actions_markdown(self) -> str:
        # Cached actions metadata to avoid repeated WorkflowController instantiation

# Unified JSON Response Processing
def _extract_json_from_response(self, response_text: str) -> str:
    # Handles ```json blocks, generic ``` blocks, and raw JSON
    # Single utility for all LLM response parsing needs
```

### 3. Advanced DOM Content Extraction
**Native XPath Evaluation** with unified processing and performance optimization.

```python
# Unified XPath Query System
async def _query_elements_by_xpath(self, context, xpath: str, is_relative: bool = False):
    # Single method for both page-level and element-level XPath queries
    # Automatic detection of text nodes and attribute XPath expressions
    # Centralized error handling with context-aware logging
    # Supports both absolute (//*[@id='...']) and relative (.//span) XPath

# Streamlined Text Node Processing
async def _find_elements_by_xpath(self, container, xpath: str):
    # Unified handling for /text() suffix XPath expressions
    # Eliminates duplicate logic between different XPath processing methods
    # Consistent behavior for text node extraction

# Performance-Optimized Fallback Selectors
@lru_cache(maxsize=128)
def _generate_fallback_selectors(self, field_name: str) -> Tuple[str, ...]:
    # Cached fallback selector generation to avoid repeated computation
    # Tuple return type for immutability and hashability
    # Support for both underscore and hyphen field name variations

# Simplified Multi-Strategy Element Selection
async def _find_field_element_with_fallback(self, container, field):
    # 1. Unified XPath processing (replaces complex if-else logic)
    # 2. CSS selector fallback with consistent error handling
    # 3. Cached fallback selectors based on field name patterns
```

### 4. Enhanced Self-Healing Execution
**Intelligent Element Selection** with native XPath evaluation and fallback strategies.

```python
# Advanced Element Selection Pipeline
async def get_best_element_handle(page, css_selector, params, timeout_ms):
    # 1. Primary CSS selector with timeout
    # 2. XPath selector from params.xpath if available
    # 3. Attribute-based targeting (data-*, id, class)
    # 4. Text content matching with normalization
    # 5. Generated fallback selectors based on element context
    # 6. Browser Use agent as final fallback

# Native XPath Support with Validation
def _is_text_xpath(self, xpath: str) -> bool:
    # Detects text nodes: /text(), /following-sibling::text()
    # Detects attributes: /@href, /@class
    # Ensures proper evaluation strategy selection
```

### 5. Structured Content Extraction
**LLM-Powered Content Understanding** with Pydantic schema validation.

```python
# Type-Safe Content Extraction
@dataclass
class ProductInfo:
    name: str
    price: float
    rating: Optional[float]

result = await controller.extract_page_content(
    "Extract product information",
    output_schema=ProductInfo
)
```

## Configuration

### 1. Environment Variables
```bash
# Required
OPENAI_API_KEY=your_openai_api_key

# Optional  
RECORDING_SERVER_PORT=7331
BROWSER_HEADLESS=false
```

### 2. Service Configuration
```python
# RecordingService Settings
USER_DATA_DIR = SCRIPT_DIR / 'user_data_dir'
EXT_DIR = SCRIPT_DIR.parent.parent.parent / 'extension' / '.output' / 'chrome-mv3'

# BuilderService Settings  
DEFAULT_LLM_MODEL = "gpt-4o"
EXTRACTION_LLM_MODEL = "gpt-4o-mini"
```

### 3. Browser Configuration
```python
# Browser Profile for Extension Loading
BrowserProfile(
    headless=False,
    user_data_dir=str(USER_DATA_DIR),
    args=[
        f'--disable-extensions-except={EXT_DIR}',
        f'--load-extension={EXT_DIR}',
        '--no-default-browser-check'
    ]
)
```

## File Structure

```
workflows/
â”œâ”€â”€ cli.py                     # Command-line interface
â”œâ”€â”€ backend/                   # FastAPI web server
â”‚   â”œâ”€â”€ api.py                # Main API routes
â”‚   â””â”€â”€ service.py            # Backend service logic
â”œâ”€â”€ workflow_use/
â”‚   â”œâ”€â”€ recorder/             # Recording service
â”‚   â”‚   â”œâ”€â”€ service.py        # Browser recording coordination
â”‚   â”‚   â””â”€â”€ views.py          # HTTP event schemas
â”‚   â”œâ”€â”€ builder/              # Workflow building service
â”‚   â”‚   â”œâ”€â”€ service.py        # LLM-powered workflow generation
â”‚   â”‚   â””â”€â”€ prompts.py        # LLM prompt templates
â”‚   â”œâ”€â”€ controller/           # Workflow execution
â”‚   â”‚   â”œâ”€â”€ service.py        # Enhanced browser controller
â”‚   â”‚   â””â”€â”€ utils.py          # Utility functions
â”‚   â”œâ”€â”€ workflow/             # Workflow processing
â”‚   â”‚   â”œâ”€â”€ service.py        # Workflow orchestration
â”‚   â”‚   â””â”€â”€ views.py          # Workflow execution schemas
â”‚   â”œâ”€â”€ schema/               # Pydantic schemas
â”‚   â”‚   â””â”€â”€ views.py          # Core data models
â”‚   â””â”€â”€ mcp/                  # Model Context Protocol
â”‚       â””â”€â”€ service.py        # MCP integration
â”œâ”€â”€ examples/                 # Sample workflows
â””â”€â”€ tmp/                      # Temporary files and recordings
```

## Integration Points

### 1. Browser Extension Integration
- **Real-time Communication**: HTTP event streaming from extension
- **Schema Alignment**: TypeScript/Pydantic type synchronization  
- **Event Processing**: Seamless workflow data aggregation

### 2. LLM Integration
- **OpenAI GPT-4o**: Workflow analysis and generation
- **Vision Capabilities**: Screenshot understanding for context
- **Structured Output**: Pydantic schema enforcement

### 3. Browser Use Integration
- **Enhanced Controller**: Extended BrowserController with workflow features
- **Agent Fallbacks**: Automatic fallback to Browser Use agent
- **Deterministic + Agentic**: Hybrid automation approach

### 4. MCP Integration
- **Tool Exposure**: Workflows as MCP tools for Claude integration
- **Service Discovery**: Automatic workflow tool registration
- **Parameter Handling**: Dynamic input schema management

This workflow engine provides the intelligent automation backbone for the Workflow Use system, combining deterministic browser automation with LLM-powered workflow generation and execution capabilities.